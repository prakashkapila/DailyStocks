Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
18/07/20 15:17:34 INFO SparkContext: Running Spark version 2.2.0
18/07/20 15:17:35 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
18/07/20 15:17:36 INFO SparkContext: Submitted application: MongoConnectorStock
18/07/20 15:17:36 INFO SecurityManager: Changing view acls to: kapila
18/07/20 15:17:36 INFO SecurityManager: Changing modify acls to: kapila
18/07/20 15:17:36 INFO SecurityManager: Changing view acls groups to: 
18/07/20 15:17:36 INFO SecurityManager: Changing modify acls groups to: 
18/07/20 15:17:36 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(kapila); groups with view permissions: Set(); users  with modify permissions: Set(kapila); groups with modify permissions: Set()
18/07/20 15:17:38 INFO Utils: Successfully started service 'sparkDriver' on port 53140.
18/07/20 15:17:38 INFO SparkEnv: Registering MapOutputTracker
18/07/20 15:17:38 INFO SparkEnv: Registering BlockManagerMaster
18/07/20 15:17:38 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
18/07/20 15:17:38 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
18/07/20 15:17:38 INFO DiskBlockManager: Created local directory at C:\Users\kapila\AppData\Local\Temp\blockmgr-0a3433b9-29a3-46d6-b386-49f48be21b4b
18/07/20 15:17:39 INFO MemoryStore: MemoryStore started with capacity 900.6 MB
18/07/20 15:17:39 INFO SparkEnv: Registering OutputCommitCoordinator
18/07/20 15:17:40 INFO Utils: Successfully started service 'SparkUI' on port 4040.
18/07/20 15:17:40 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://169.254.228.70:4040
18/07/20 15:17:41 INFO Executor: Starting executor ID driver on host localhost
18/07/20 15:17:41 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 53161.
18/07/20 15:17:41 INFO NettyBlockTransferService: Server created on 169.254.228.70:53161
18/07/20 15:17:41 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
18/07/20 15:17:41 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 169.254.228.70, 53161, None)
18/07/20 15:17:41 INFO BlockManagerMasterEndpoint: Registering block manager 169.254.228.70:53161 with 900.6 MB RAM, BlockManagerId(driver, 169.254.228.70, 53161, None)
18/07/20 15:17:41 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 169.254.228.70, 53161, None)
18/07/20 15:17:41 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 169.254.228.70, 53161, None)
18/07/20 15:17:42 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/C:/stocks/DailyStocks/spark-warehouse/').
18/07/20 15:17:42 INFO SharedState: Warehouse path is 'file:/C:/stocks/DailyStocks/spark-warehouse/'.
18/07/20 15:17:47 INFO StateStoreCoordinatorRef: Registered StateStoreCoordinator endpoint
18/07/20 15:17:47 WARN SparkSession$Builder: Using an existing SparkSession; some configuration may not take effect.
18/07/20 15:17:48 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 208.0 B, free 900.6 MB)
18/07/20 15:17:48 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 401.0 B, free 900.6 MB)
18/07/20 15:17:48 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 169.254.228.70:53161 (size: 401.0 B, free: 900.6 MB)
18/07/20 15:17:48 INFO SparkContext: Created broadcast 0 from broadcast at MongoSpark.scala:536
Exception in thread "main" java.lang.IllegalArgumentException: Field "features" does not exist.
	at org.apache.spark.sql.types.StructType$$anonfun$apply$1.apply(StructType.scala:266)
	at org.apache.spark.sql.types.StructType$$anonfun$apply$1.apply(StructType.scala:266)
	at scala.collection.MapLike$class.getOrElse(MapLike.scala:128)
	at scala.collection.AbstractMap.getOrElse(Map.scala:59)
	at org.apache.spark.sql.types.StructType.apply(StructType.scala:265)
	at org.apache.spark.ml.util.SchemaUtils$.checkColumnType(SchemaUtils.scala:40)
	at org.apache.spark.ml.clustering.KMeansParams$class.validateAndTransformSchema(KMeans.scala:93)
	at org.apache.spark.ml.clustering.KMeans.validateAndTransformSchema(KMeans.scala:254)
	at org.apache.spark.ml.clustering.KMeans.transformSchema(KMeans.scala:340)
	at org.apache.spark.ml.PipelineStage.transformSchema(Pipeline.scala:74)
	at org.apache.spark.ml.clustering.KMeans.fit(KMeans.scala:305)
	at com.stocks.DailyStocks.ml.StockLinearRegressionClassification.performKmeans(StockLinearRegressionClassification.java:108)
	at com.stocks.DailyStocks.ml.StockLinearRegressionClassification.perform(StockLinearRegressionClassification.java:103)
	at com.stocks.DailyStocks.ml.StockLinearRegressionClassification.main(StockLinearRegressionClassification.java:120)
18/07/20 15:17:51 INFO SparkContext: Invoking stop() from shutdown hook
18/07/20 15:17:51 INFO SparkUI: Stopped Spark web UI at http://169.254.228.70:4040
18/07/20 15:17:51 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
18/07/20 15:17:51 INFO MemoryStore: MemoryStore cleared
18/07/20 15:17:51 INFO BlockManager: BlockManager stopped
18/07/20 15:17:51 INFO BlockManagerMaster: BlockManagerMaster stopped
18/07/20 15:17:51 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
18/07/20 15:17:51 INFO SparkContext: Successfully stopped SparkContext
18/07/20 15:17:51 INFO ShutdownHookManager: Shutdown hook called
18/07/20 15:17:51 INFO ShutdownHookManager: Deleting directory C:\Users\kapila\AppData\Local\Temp\spark-48a234ce-e744-4cde-9011-af5ba4db4c07
